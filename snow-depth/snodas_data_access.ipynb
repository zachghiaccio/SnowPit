{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e721c4-5532-491c-a16b-cc317951ffe6",
   "metadata": {},
   "source": [
    "# SNODAS Data Access\n",
    "\n",
    "This script is designed to access and process data from the Snow Data Assimilation (SNODAS) system. \n",
    "\n",
    "Data is accessed through NSIDC. Because SNODAS is not available through the cloud, we must use HTTPS data querying to download and process the data.\n",
    "\n",
    "This script is adapted from code written by Aakash Ahamed (https://github.com/kashingtonDC/SNODAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133a377-2eaf-4de6-97b0-3907195d233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import datetime\n",
    "import subprocess \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e04c1-5749-40bb-9ccb-0f7bf801b949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Year/month/day setup for SNODAS archive\n",
    "year = \"2023\"\n",
    "month = \"Mar\" # 3-character abbreviation for month\n",
    "day = \"03\" # 2-digit number for day of month\n",
    "\n",
    "# Get urls for SNODAS archive\n",
    "archive_url = f'https://noaadata.apps.nsidc.org/NOAA/G02158/masked/{year}/{day}_{month}/'\n",
    "r = requests.get(archive_url)\n",
    "data = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "# Extract data from SNODAS archive\n",
    "dir = \"/home/jovyan/shared-public/SnowPit/tmp/\"\n",
    "for l in data.find_all(\"a\")[1:]:\n",
    "    r = requests.get(archive_url+l['href'])\n",
    "    with open(os.path.join(dir, l['href']), 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597b7bd-7f00-4d43-b5f8-cbf543df0291",
   "metadata": {},
   "source": [
    "Using the above cell, we are able to access all of the SNODAS data from March 2023. However, said data is provided in `.tar` format, which must then be extracted from the `.gz` format. The below function extracts the data we need to `.dat`, and `.txt` formats.\n",
    "\n",
    "Notice also the `snovars` input: the given input (`1034`) extracts the SNODAS snow water equivalent (SWE) variable from each tar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b45ae-664c-4c61-aac9-c525def854a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tarfile(tarfile, writedir, snovars=['1034']):\n",
    "    # Extract date from tarfile\n",
    "    date = os.path.splitext(os.path.split(tarfile)[1])[0].replace(\"SNODAS_\",\"\")\n",
    "    \n",
    "    # Untar the files using OS commands\n",
    "    cmd = '''tar -xvf {} -C {}'''.format(tarfile, writedir)\n",
    "    os.system(cmd)\n",
    "\n",
    "    # Find untarred .gz files\n",
    "    gz_files = [os.path.join(writedir,x) for x in os.listdir(writedir) if date in x if x.endswith(\".gz\")]\n",
    "\n",
    "    # Get variable strings from each file\n",
    "    varstrs = [x[x.find(\"ssmv\")+5:x.find(\"ssmv\")+9] for x in gz_files]\n",
    "\n",
    "    # Compare variable strings to wanted variables\n",
    "    for varstr,file in zip(varstrs, gz_files):\n",
    "        outfn = os.path.splitext(file)[0]\n",
    "        if varstr in snovars:\n",
    "            with gzip.open(file, 'r') as f_in, open(outfn, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    datfiles = [os.path.join(writedir,x) for x in os.listdir(writedir) if date in x if x.endswith(\".dat\")]\n",
    "    txtfiles = [os.path.join(writedir,x) for x in os.listdir(writedir) if date in x if x.endswith(\".txt\")]\n",
    "    gz_files = [os.path.join(writedir,x) for x in os.listdir(writedir) if date in x if x.endswith(\".gz\")]\n",
    "\n",
    "    return datfiles, txtfiles, gz_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db73909-c5eb-425b-8ddf-1301336a1378",
   "metadata": {},
   "source": [
    "In order to interpret the `.dat` files in Python, we need to convert the text files to the ENVI header format. This requires us to define a few parameters for the header, which were obtained from the following website: https://nsidc.org/data/user-resources/help-center/how-do-i-convert-snodas-binary-files-geotiff-or-netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7bdf0-f1bb-4d00-94f1-a39a463c9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of header parameters\n",
    "hdr_parms = {\n",
    "    \"samples\": 6935,\n",
    "    \"lines\": 3351,\n",
    "    \"bands\": 1,\n",
    "    \"data_type\": 2,\n",
    "    \"interleave\": \"bsq\",\n",
    "    \"byte_order\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735a21b-5022-42b9-a29b-2f4273e8a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_envi_header(txt_path, hdr_path, hdr_parms):\n",
    "    \"\"\"\n",
    "    Creates an ENVI header (.hdr) file.\n",
    "\n",
    "    Args:\n",
    "        txt_path (str): Path to the input .txt file (used only for the 'map info' field).\n",
    "        hdr_path (str): Path to save the output .hdr file.\n",
    "        description (str): Description of the data.\n",
    "        samples (int): Number of samples (columns).\n",
    "        lines (int): Number of lines (rows).\n",
    "        bands (int): Number of bands.\n",
    "        data_type (int): ENVI data type code (e.g., 1 for byte, 4 for float).\n",
    "        interleave (str): Interleave type ('bsq', 'bip', or 'bil').\n",
    "        byte_order (int): Byte order (0 for little-endian, 1 for big-endian).\n",
    "    \"\"\"\n",
    "    # Open .txt files\n",
    "    with open(txt_path, 'r') as file:\n",
    "      lines_txt = file.readlines()\n",
    "    map_info_line = next((line for line in lines_txt if \"map info\" in line.lower()), None)\n",
    "\n",
    "    # Write header parameters to .hdr file\n",
    "    with open(hdr_path, 'w') as hdr_file:\n",
    "        hdr_file.write(\"ENVI\\n\")\n",
    "        hdr_file.write(f\"samples = {hdr_parms['samples']}\\n\")\n",
    "        hdr_file.write(f\"lines = {hdr_parms['lines']}\\n\")\n",
    "        hdr_file.write(f\"bands = {hdr_parms['bands']}\\n\")\n",
    "        hdr_file.write(f\"header offset = 0\\n\")\n",
    "        hdr_file.write(f\"file type = ENVI Standard\\n\")\n",
    "        hdr_file.write(f\"data type = {hdr_parms['data_type']}\\n\")\n",
    "        hdr_file.write(f\"interleave = {hdr_parms['interleave']}\\n\")\n",
    "        hdr_file.write(f\"sensor type = Unknown\\n\")\n",
    "        hdr_file.write(f\"byte order = {hdr_parms['byte_order']}\\n\")\n",
    "        print(f\"Saving header to file: {hdr_path}\")\n",
    "        if map_info_line:\n",
    "          hdr_file.write(f\"{map_info_line}\")\n",
    "        else:\n",
    "          hdr_file.write(f\"map info = {{UTM, 1.000, 1.000, 0.000, 0.000, 1, 1, WGS-84, units=Meters}}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59829467-af57-4dfc-afe3-ebe3128912de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_dir = \"/home/jovyan/shared-public/SnowPit/tmp/\"\n",
    "# Create ENVI headers, and save to tmp directory\n",
    "for file in os.listdir(tmp_dir):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(tmp_dir, file)\n",
    "        hdr_path = f\"{file_path[:-4]}.hdr\"\n",
    "        create_envi_header(file_path, hdr_path, hdr_parms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48038a1-12e8-45e7-8308-59d41074f5fe",
   "metadata": {},
   "source": [
    "Now that we have both `.dat` and `.hdr` files, we can finally convert them to GeoTiffs for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3a3de-37db-4c92-a9f5-148e17265414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dat2tif(datfiles, writedir):\n",
    "    # Set dicionary of desired parameters\n",
    "    prod_lookup = dict({\n",
    "        \"1034\": \"SNWE\"\n",
    "    })\n",
    "\n",
    "    outfnsv1 = {}\n",
    "\n",
    "    # Create output tiff file for snow output\n",
    "    for file in datfiles:\n",
    "        date = file[file.find(\"TS\")+2:file.find(\"TS\")+10]\n",
    "        for k,v in prod_lookup.items():\n",
    "            if k in file:\n",
    "                outfnsv1[file] = date + v + \".tif\"\n",
    "\n",
    "    # Create path for saved file\n",
    "    outfnsvf = {}\n",
    "    for k,v in outfnsv1.items():\n",
    "        outfnsvf[k] = os.path.join(writedir, v)\n",
    "\n",
    "    # Use GDAL to convert .dat and .hdr to .tif\n",
    "    outfiles = []\n",
    "    for infile,outfile in outfnsvf.items():\n",
    "        if not os.path.exists(outfile):\n",
    "            cmd = '''gdal_translate -of GTIff -a_srs '+proj=longlat +ellps=WGS84 +no_defs' -a_nodata -9999 -a_ullr -124.73333333 52.87500000 -66.94166667 24.95000000 {} {}'''.format(infile,outfile)\n",
    "            os.system(cmd)\n",
    "        else:\n",
    "            print(\"{} already exists - moving to next file\".format(outfile))\n",
    "\n",
    "        outfiles.append(outfile)\n",
    "\n",
    "    return outfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb08c02-991d-41fc-88fb-c84ab529d557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look through .dat files for conversion to .tif\n",
    "for file in os.listdir(tmp_dir):\n",
    "    if file.endswith(\".dat\"):\n",
    "        print(file)\n",
    "        dat_path = os.path.join(tmp_dir, file)\n",
    "        tiff_path = f\"{dat_path[:-4]}.tif\"\n",
    "        tiffile = dat2tif([dat_path], tmp_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc7fd6-b9ab-4c3c-91b7-0c8b9f875340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up tmp directory, removing excess .tar, .gz, .txt, .dat, .hdr files\n",
    "import os\n",
    "\n",
    "tmp_dir = \"/home/jovyan/shared-public/SnowPit/tmp/\"\n",
    "files = os.listdir(tmp_dir)\n",
    "\n",
    "for f in files:\n",
    "    file_path = os.path.join(tmp_dir, f)\n",
    "    if os.path.isfile(file_path) and not file_path.endswith(\".tif\"):\n",
    "        os.remove(file_path)\n",
    "\n",
    "print(\"Excess files deleted. Only Tiffs remaining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c1dd4-de9f-4bc4-ab0d-2919228daaa7",
   "metadata": {},
   "source": [
    "It took a fair bit of effort, but we finally have some usable SNODAS data. Let's take a look at one of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a1ca8-d2bc-43bd-9441-d76417cacfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "\n",
    "ds = rxr.open_rasterio(\"/home/jovyan/shared-public/SnowPit/tmp/20230301SNWE.tif\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119af81-868c-4125-b019-ad80530e1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.plot(vmin=0, vmax=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a16dd-03d6-4f81-bf15-2a928d925850",
   "metadata": {},
   "source": [
    "The values look a bit large for SWE...that's because we need to apply a scale factor to the data to have proper values.\n",
    "\n",
    "According to the SNODAS User Guide (https://nsidc.org/sites/default/files/g02158-v001-userguide_2_1.pdf), the scale factor for SWE is 1000, which converts the values to units of meters. If desired, the above values can be kept to represent SWE in millimeters (mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e86b5e-cd22-4010-ab84-f72d84de0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SWE data to meters\n",
    "ds_meters = ds/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de43c8e-2ab8-4fa3-a254-5d02e74a32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot SWE data over continental U.S.\n",
    "fig, ax = plt.subplots()\n",
    "ds_meters.where(ds>0).plot(vmin=0, vmax=1, cbar_kwargs={'label': \"SWE [mm]\"})\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\" \")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
